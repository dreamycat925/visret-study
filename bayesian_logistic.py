import pymc as pm
import numpy as np
import pandas as pd
import arviz as az
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_curve, auc

# ðŸ”¹ NumPy ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®šï¼ˆPyMC ã®ãƒ©ãƒ³ãƒ€ãƒ æ€§ã«ã‚‚å½±éŸ¿ï¼‰
np.random.seed(42)

# âœ… APç¾¤ã‚’é™¤å¤–ã—ã€HP vs AD ã®æ¯”è¼ƒã‚’è¡Œã†
Z = df_com[df_com['group'].isin(['hs', 'ci'])].copy()

# âœ… èª¬æ˜Žå¤‰æ•°ï¼ˆã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’ãƒ€ãƒŸãƒ¼åŒ–ï¼‰
X = Z[['æ¤œæŸ»æ™‚ã®å¹´é½¢', 'æ€§åˆ¥', 'æ•™è‚²æ­´', 'çµµã®å†èªèª²é¡Œ_ç‚¹æ•°', 'çµµã®å†èªèª²é¡Œ_è™šå†èªã®æ•°']]
X = pd.get_dummies(X, columns=['æ€§åˆ¥'], drop_first=True, dtype=int)  

# âœ… ç›®çš„å¤‰æ•°ï¼ˆAD = 1, HP = 0ï¼‰
y = (Z['group'] == 'ci').astype(int).values  

# æ¨™æº–åŒ–
num_cols = ['æ¤œæŸ»æ™‚ã®å¹´é½¢', 'æ•™è‚²æ­´', 'çµµã®å†èªèª²é¡Œ_ç‚¹æ•°', 'çµµã®å†èªèª²é¡Œ_è™šå†èªã®æ•°']
scaler = StandardScaler()
X[num_cols] = scaler.fit_transform(X[num_cols])

# âœ… PyMCã«ã‚ˆã‚‹ãƒ™ã‚¤ã‚ºãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›žå¸°
with pm.Model() as bayesian_logit_model:
    Î²_age = pm.Normal("Î²_age", mu=0, sigma=1)
    Î²_edu = pm.Normal("Î²_edu", mu=0, sigma=1)
    Î²_gender = pm.Normal("Î²_gender", mu=0, sigma=1)
    Î²_score = pm.Normal("Î²_score", mu=0, sigma=1)
    Î²_fal_recog = pm.Normal("Î²_fal_recog", mu=0, sigma=1)
    intercept = pm.Normal("intercept", mu=0, sigma=2)
    
    # ç·šå½¢çµåˆ
    Î¼ = (
        intercept
        + Î²_age * X["æ¤œæŸ»æ™‚ã®å¹´é½¢"].values
        + Î²_edu * X["æ•™è‚²æ­´"].values
        + Î²_gender * X["æ€§åˆ¥_ç”·"].values
        + Î²_score * X['çµµã®å†èªèª²é¡Œ_ç‚¹æ•°'].values
        + Î²_fal_recog * X['çµµã®å†èªèª²é¡Œ_è™šå†èªã®æ•°'].values
    )
    
    # ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã§ç¢ºçŽ‡ã‚’å¾—ã‚‹
    p = pm.math.sigmoid(Î¼)
    
    # ãƒ™ãƒ«ãƒŒãƒ¼ã‚¤åˆ†å¸ƒã®å°¤åº¦
    likelihood = pm.Bernoulli("y", p=p, observed=y)
    
    # âœ… MCMCã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆã‚¨ãƒ©ãƒ¼ã‚’é˜²ããŸã‚ã€ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’èª¿æ•´ï¼‰
    trace_bayes_logit = pm.sample(2000, tune=1000, target_accept=0.98, return_inferencedata=True)

# âœ… äº‹å¾Œåˆ†å¸ƒã®è¦ç´„
summary = az.summary(trace_bayes_logit, stat_funcs={"median": np.median}, hdi_prob=0.95)
print(summary)

# âœ… ã‚ªãƒƒã‚ºæ¯”ã‚’è¨ˆç®—
odds_ratios = np.exp(summary['median'])
print("\nã‚ªãƒƒã‚ºæ¯”:")
print(odds_ratios)

odds_ratios_lower = np.exp(summary['hdi_2.5%'])
print("\nã‚ªãƒƒã‚ºæ¯”ä¸‹é™:")
print(odds_ratios_lower)

odds_ratios_upper = np.exp(summary['hdi_97.5%'])
print("\nã‚ªãƒƒã‚ºæ¯”ä¸Šé™:")
print(odds_ratios_upper)

# âœ… äº‹å¾Œç¢ºçŽ‡ã‚’è¨ˆç®—
def compute_posterior_probabilities(trace, param):
    """äº‹å¾Œç¢ºçŽ‡ã‚’æ±‚ã‚ã‚‹"""
    samples = trace.posterior[param].values.flatten()
    prob_positive = (samples > 0).mean()
    prob_negative = (samples < 0).mean()
    return prob_positive, prob_negative

print("\näº‹å¾Œç¢ºçŽ‡:")
for param in ["Î²_age", "Î²_edu", "Î²_gender", "Î²_score", "Î²_fal_recog"]:
    p_pos, p_neg = compute_posterior_probabilities(trace_bayes_logit, param)
    print(f"{param}: P(Î² > 0) = {p_pos:.3f}, P(Î² < 0) = {p_neg:.3f}")

# âœ… äºˆæ¸¬ç¢ºçŽ‡ã‚’å–å¾—ï¼ˆã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£ï¼‰
with bayesian_logit_model:
    posterior_pred = pm.sample_posterior_predictive(trace_bayes_logit, var_names=["y"])

# âœ… äº‹å¾Œåˆ†å¸ƒã®å¹³å‡ã‚’è¨ˆç®—ï¼ˆã‚¨ãƒ©ãƒ¼ã‚’é˜²ããŸã‚ã€`posterior_predictive` ã®æ§‹é€ ã‚’ç¢ºèªï¼‰
pred_prob = posterior_pred.posterior_predictive["y"].mean(dim=["chain", "draw"]).values  

# âœ… ROCæ›²ç·šã®ä½œæˆ
fpr, tpr, thresholds = roc_curve(y, pred_prob)  
roc_auc = auc(fpr, tpr)
print(f'AUC: {roc_auc}')

# âœ… ãƒ—ãƒ­ãƒƒãƒˆ
plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('1 - Specificity')
plt.ylabel('Sensitivity')
plt.title('ROC Curve')

# ç›®ç››ã‚Šã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã¨å¤ªã•ã‚’èª¿æ•´
plt.tick_params(axis='both', which='major', labelsize=22, width=2)

# ç›®ç››ã‚Šãƒ©ãƒ™ãƒ«ã‚’å¤ªå­—ã«ã™ã‚‹
for label in plt.gca().get_xticklabels():
    label.set_fontweight('bold')
for label in plt.gca().get_yticklabels():
    label.set_fontweight('bold')
    
plt.legend()
plt.tight_layout()
plt.show()
